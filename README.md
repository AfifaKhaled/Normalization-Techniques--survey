# Normalization-Techniques--survey


 Layer Normalization
 paper  Layer Normalization
 
 Batch Normalization
 Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift

 Instance Normalization
 Instance Normalization: The Missing Ingredient for Fast Stylization

 Local Response Normalization
 ImageNet Classification with Deep Convolutional Neural Networks

 Adaptive Instance Normalization
 Arbitrary Style Transfer in Real-time with Adaptive Instance Normalization

 Weight Demodulation
 Analyzing and Improving the Image Quality of StyleGAN

 Spectral Normalization
 Spectral Normalization for Generative Adversarial Networks

 Conditional Batch Normalization
 Modulating early visual processing by language

 Weight Normalization
 Weight Normalization: A Simple Reparameterization to Accelerate Training of Deep Neural Networks

 Group Normalization
 Group Normalization

 Activation Normalization
 Glow: Generative Flow with Invertible 1x1 Convolutions


 Semantic Image Synthesis with Spatially-Adaptive Normalization

 
 Stable Rank Normalization for Improved Generalization in Neural Networks and GANs


 Is Second-order Information Helpful for Large-scale Visual Recognition?

 LayerScale
 Going deeper with Image Transformers

 Switchable Normalization
 Differentiable Learning-to-Normalize via Switchable Normalization

 Weight Standardization
 Micro-Batch Training with Batch-Channel Normalization and Weight Standardization

 Local Contrast Normalization

 Gradient Normalization
 Gradient Normalization for Generative Adversarial Networks

 SyncBN
 Context Encoding for Semantic Segmentation

 Decorrelated Batch Normalization
 Decorrelated Batch Normalization

 ReZero
 ReZero is All You Need: Fast Convergence at Large Depth
	
 Attentive Normalization
 
 Conditional Instance Normalization
 
 Online Normalization
 Online Normalization for Training Neural Networks

 RMSNorm
 Root Mean Square Layer Normalization

 Cosine Normalization
 Cosine Normalization: Using Cosine Similarity Instead of Dot Product in Neural Networks

 Filter Response Normalization
 Filter Response Normalization Layer: Eliminating Batch Dependence in the Training of Deep Neural Networks

 InPlace-ABN
 In-Place Activated BatchNorm for Memory-Optimized Training of DNNs

 BatchChannel Normalization
 Rethinking Normalization and Elimination Singularity in Neural Networks

 Instance-Level Meta Normalization
 Instance-Level Meta Normalization

 Mixture Normalization
 Training Faster by Separating Modes of Variation in Batch-normalized Models

 Mode Normalization
 Mode Normalization

 Sparse Switchable Normalization
 SSN: Learning Sparse Switchable Normalization via SparsestMax

 Virtual Batch Normalization
 Improved Techniques for Training GANs

 SaBN
 Sandwich Batch Normalization: A Drop-In Replacement for Feature Distribution Heterogeneity

 EvoNorms
 Evolving Normalization-Activation Layers



